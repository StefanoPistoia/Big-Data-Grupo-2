{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "Dh8MkXaG-c9Y",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Big Data y Machine Learning (UBA) -  2025\n",
    "\n",
    "## Trabajo Práctico 4: Clasificación de pobres "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RhBlm6mZ-c9e"
   },
   "source": [
    "### Reglas de formato y presentación\n",
    "- El trabajo debe estar debidamente documentado comentado (utilizando #) para que tanto los docentes como sus compañeros puedan comprender el código fácilmente.\n",
    "\n",
    "- El mismo debe ser completado en este Jupyter Notebook y entregado como tal, es decir en un archivo .ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZEjGaa4U-c9g"
   },
   "source": [
    "### Fecha de entrega:\n",
    "<font color=red>Martes 11 de Noviembre a las 13:00 hs</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enfoque de validación "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXbrPraa-c9i"
   },
   "source": [
    "Cargamos las bases previamente limpiadas, las separamos y seleccionamos variables de interés para la clasificación de pobres con su explicación correspondiente en el informe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de paquetes\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "from docx import Document\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "from docx.shared import Pt\n",
    "from docx.oxml.ns import qn\n",
    "from docx.oxml import OxmlElement\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el archivo Excel en un DataFrame de pandas\n",
    "df = pd.read_excel(\"db_nea_respuestas.xlsx\")\n",
    "\n",
    "# Nombre de la columna que contiene el año\n",
    "columna_anio = 'ANO4'\n",
    "# Semilla para la reproducibilidad\n",
    "random_seed = 444\n",
    "\n",
    "# --- Procesamiento para el año 2005 ---\n",
    "respondieron_2005 = df[df[columna_anio] == 2005]\n",
    "\n",
    "train_2005, test_2005 = train_test_split(\n",
    "    respondieron_2005,\n",
    "    test_size=0.3,\n",
    "    random_state=random_seed\n",
    ")\n",
    "\n",
    "# --- Procesamiento para el año 2025 ---\n",
    "\n",
    "respondieron_2025 = df[df[columna_anio] == 2025]\n",
    "\n",
    "train_2025, test_2025 = train_test_split(\n",
    "    respondieron_2025,\n",
    "    test_size=0.3,\n",
    "    random_state=random_seed\n",
    ")\n",
    "\n",
    "# Definimos variables\n",
    "Y = 'pobre'\n",
    "X = ['horastrab','educ','edad','edad2','cobertura_medica','sexo','ESTADO','estado_civil']\n",
    "\n",
    "# --- Tabla de diferencia de medias con p-values entre train y test para cada año ---\n",
    "\n",
    "results = []\n",
    "\n",
    "# Diccionario para mapear el año a sus dataframes de train y test\n",
    "dataframes_por_anio = {\n",
    "    2005: {'train': train_2005, 'test': test_2005},\n",
    "    2025: {'train': train_2025, 'test': test_2025}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Procesando año: 2005\n",
      "  Procesando año: 2025\n",
      "\n",
      "Tabla de Diferencia de Medias y P-values:\n",
      "     Año          Variable  Media_Train   Media_Test  Diferencia_Medias   P_Value\n",
      "0   2005         horastrab    13.306290    13.684957          -0.378667  0.601380\n",
      "1   2005              educ     8.387192     8.154006           0.233186  0.100091\n",
      "2   2005              edad    29.070560    28.632552           0.438008  0.471905\n",
      "3   2005             edad2  1269.653013  1238.689273          30.963740  0.500475\n",
      "4   2005  cobertura_medica     2.642838     2.726485          -0.083647  0.088158\n",
      "5   2005              sexo     1.519820     1.514797           0.005024  0.734900\n",
      "6   2005            ESTADO     2.467460     2.451295           0.016166  0.634320\n",
      "7   2005      estado_civil     3.744186     3.749075          -0.004889  0.916227\n",
      "8   2025         horastrab    36.737194    35.926768           0.810426  0.456835\n",
      "9   2025              educ     9.759351     9.857749          -0.098398  0.580023\n",
      "10  2025              edad    35.852332    34.917588           0.934744  0.251287\n",
      "11  2025             edad2  1755.186528  1677.008040          78.178488  0.236178\n",
      "12  2025  cobertura_medica     2.223478     2.261352          -0.037874  0.777534\n",
      "13  2025              sexo     1.533362     1.532934           0.000428  0.981896\n",
      "14  2025            ESTADO     2.337896     2.320359           0.017536  0.680172\n",
      "15  2025      estado_civil     3.535928     3.596806          -0.060878  0.332635\n"
     ]
    }
   ],
   "source": [
    "# Crear un loop para calcular las medias en una tabla\n",
    "for anio, dfs in dataframes_por_anio.items():\n",
    "    train_df = dfs['train']\n",
    "    test_df = dfs['test']\n",
    "    \n",
    "    print(f\"  Procesando año: {anio}\")\n",
    "    \n",
    "    for var in X:\n",
    "        # Asegurarse de que la variable existe en ambos dataframes\n",
    "        if var in train_df.columns and var in test_df.columns:\n",
    "            # Calcular medias\n",
    "            mean_train = train_df[var].mean()\n",
    "            mean_test = test_df[var].mean()\n",
    "            \n",
    "            # Calcular diferencia de medias\n",
    "            diff_mean = mean_train - mean_test\n",
    "            \n",
    "            # Realizar t-test de Student para muestras independientes\n",
    "            # Usamos .dropna() para manejar posibles valores faltantes\n",
    "            # equal_var=False para Welch's t-test, que no asume varianzas iguales\n",
    "            t_stat, p_value = stats.ttest_ind(\n",
    "                train_df[var].dropna(), \n",
    "                test_df[var].dropna(), \n",
    "                equal_var=False\n",
    "            )\n",
    "            \n",
    "            results.append({\n",
    "                'Año': anio,\n",
    "                'Variable': var,\n",
    "                'Media_Train': mean_train,\n",
    "                'Media_Test': mean_test,\n",
    "                'Diferencia_Medias': diff_mean,\n",
    "                'P_Value': p_value\n",
    "            })\n",
    "        else:\n",
    "            print(f\"    Advertencia: La variable '{var}' no se encontró en los dataframes del año {anio}. Se omitirá.\")\n",
    "\n",
    "# Crear un DataFrame con los resultados\n",
    "df_medias_pvalues = pd.DataFrame(results)\n",
    "\n",
    "# Mostrar el DataFrame resultante\n",
    "print(\"\\nTabla de Diferencia de Medias y P-values:\")\n",
    "print(df_medias_pvalues)\n",
    "\n",
    "# Exportar a Excel\n",
    "df_medias_pvalues.to_excel(\"diferencia_medias_train_test.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte B - Regresión Logística\n",
    "Procesamos y preparamos las variables seleccionadas previamente y corremos el modelo exportando la tabla a DOCX y graficando los resultados con años de educación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.564313\n",
      "         Iterations 6\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                  pobre   No. Observations:                  878\n",
      "Model:                          Logit   Df Residuals:                      864\n",
      "Method:                           MLE   Df Model:                           13\n",
      "Date:                Tue, 04 Nov 2025   Pseudo R-squ.:                  0.1668\n",
      "Time:                        14:06:49   Log-Likelihood:                -495.47\n",
      "converged:                       True   LL-Null:                       -594.65\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.957e-35\n",
      "=========================================================================================\n",
      "                            coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------\n",
      "const                     0.3832      0.903      0.424      0.671      -1.387       2.153\n",
      "horastrab                -0.0169      0.004     -3.762      0.000      -0.026      -0.008\n",
      "educ                     -0.1574      0.025     -6.229      0.000      -0.207      -0.108\n",
      "edad                      0.0943      0.041      2.297      0.022       0.014       0.175\n",
      "edad2                    -0.0014      0.000     -2.805      0.005      -0.002      -0.000\n",
      "cobertura_medica_2.0     -0.9181      1.092     -0.840      0.401      -3.059       1.223\n",
      "cobertura_medica_3.0      1.6558      0.343      4.828      0.000       0.984       2.328\n",
      "cobertura_medica_4.0      1.2644      0.179      7.063      0.000       0.913       1.615\n",
      "cobertura_medica_12.0     0.7596      1.436      0.529      0.597      -2.054       3.574\n",
      "sexo_2                   -0.4563      0.167     -2.735      0.006      -0.783      -0.129\n",
      "estado_civil_2            0.4335      0.225      1.930      0.054      -0.007       0.874\n",
      "estado_civil_3            0.8199      0.335      2.446      0.014       0.163       1.477\n",
      "estado_civil_4           -0.0761      0.633     -0.120      0.904      -1.317       1.165\n",
      "estado_civil_5           -0.5503      0.199     -2.760      0.006      -0.941      -0.159\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# --- Regresión logística ---\n",
    "train_2025_full = train_2025[[Y] + X].dropna()\n",
    "\n",
    "# Preparar los datos de entrenamiento para el modelo\n",
    "y_train = train_2025_full[Y]\n",
    "X_train = train_2025_full[X]\n",
    "\n",
    "# Convertir variables categóricas en dummies\n",
    "categorical_vars = ['cobertura_medica', 'sexo', 'ESTADO', 'estado_civil']\n",
    "X_train_dummies = pd.get_dummies(X_train, columns=categorical_vars, drop_first=True, dtype=float)\n",
    "\n",
    "# Añadir constante\n",
    "X_train_const = sm.add_constant(X_train_dummies)\n",
    "\n",
    "# Correr y entrenar el modelo Logit\n",
    "logit_model = sm.Logit(y_train, X_train_const)\n",
    "result = logit_model.fit()\n",
    "\n",
    "# Resumen del modelo\n",
    "print(result.summary())\n",
    "\n",
    "# Extraer resultados\n",
    "coef = result.params\n",
    "se = result.bse\n",
    "z = result.tvalues\n",
    "p = result.pvalues\n",
    "conf = result.conf_int()\n",
    "conf.columns = ['IC 2.5%', 'IC 97.5%']\n",
    "\n",
    "# Crear DataFrame\n",
    "tabla_resultados = pd.DataFrame({\n",
    "    'Variable': coef.index,\n",
    "    'Coef. (β)': coef.values,\n",
    "    'exp(β)': np.exp(coef.values),\n",
    "    'Error Std.': se.values,\n",
    "    'z': z.values,\n",
    "    'p-value': p.values,\n",
    "    'IC 2.5%': conf['IC 2.5%'].values,\n",
    "    'IC 97.5%': conf['IC 97.5%'].values\n",
    "})\n",
    "\n",
    "tabla_resultados = tabla_resultados.round(4)\n",
    "\n",
    "# Exportar a Word \n",
    "doc = Document()\n",
    "doc.add_heading('Resultados del modelo Logit', level=1)\n",
    "\n",
    "# Crear tabla\n",
    "t = doc.add_table(rows=1, cols=len(tabla_resultados.columns))\n",
    "t.style = 'Light List Accent 1'\n",
    "\n",
    "# Cabeceras\n",
    "hdr_cells = t.rows[0].cells\n",
    "for i, col in enumerate(tabla_resultados.columns):\n",
    "    hdr_cells[i].text = col\n",
    "\n",
    "# Filas\n",
    "for _, row in tabla_resultados.iterrows():\n",
    "    row_cells = t.add_row().cells\n",
    "    for j, val in enumerate(row):\n",
    "        cell = row_cells[j].paragraphs[0].add_run(str(val))\n",
    "        if j == 0 or tabla_resultados.loc[_, 'p-value'] < 0.05:\n",
    "            # Si es nombre de variable o p < 0.05 => negrita\n",
    "            cell.bold = True\n",
    "\n",
    "# Ajustar tamaño de fuente\n",
    "for row in t.rows:\n",
    "    for cell in row.cells:\n",
    "        for paragraph in cell.paragraphs:\n",
    "            for run in paragraph.runs:\n",
    "                run.font.size = Pt(10)\n",
    "\n",
    "# Guardar documento\n",
    "doc.save('Resultados_Logit.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico\n",
    "\n",
    "# Crear rango de valores para 'educ'\n",
    "educ_range = np.linspace(X_train_const['educ'].min(), X_train_const['educ'].max(), 100)\n",
    "\n",
    "# Crear un DataFrame con 'educ' variable y los demás fijos en su media\n",
    "X_pred = X_train_const.copy()\n",
    "for col in X_pred.columns:\n",
    "    if col != 'educ':\n",
    "        X_pred[col] = X_pred[col].mean()\n",
    "\n",
    "# Reemplazamos 'educ' por el rango deseado\n",
    "X_pred = X_pred.loc[X_pred.index.repeat(len(educ_range))].reset_index(drop=True)\n",
    "X_pred['educ'] = np.tile(educ_range, len(X_train_const))\n",
    "\n",
    "# Predecir probabilidades\n",
    "pred_probs = result.predict(X_pred)\n",
    "\n",
    "# Graficar relación entre educación y probabilidad predicha\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(educ_range, pred_probs.groupby(X_pred['educ']).mean(), color='darkblue', linewidth=2)\n",
    "plt.title('Probabilidad predicha de ser pobre según nivel educativo', fontsize=13)\n",
    "plt.xlabel('Años de educación', fontsize=12)\n",
    "plt.ylabel('Probabilidad predicha de ser pobre', fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte C - Método de Vecinos Cercanos (KNN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clasificación con K-Nearest Neighbors (KNN) \n",
    "\n",
    "# Usamos las mismas variables X e Y que en la regresión logística\n",
    "train_df_knn = train_2025[[Y] + X].dropna()\n",
    "test_df_knn = test_2025[[Y] + X].dropna()\n",
    "\n",
    "y_train_knn = train_df_knn[Y]\n",
    "X_train_knn = train_df_knn[X]\n",
    "\n",
    "y_test_knn = test_df_knn[Y]\n",
    "X_test_knn = test_df_knn[X]\n",
    "\n",
    "# Convertir variables categóricas en dummies\n",
    "X_train_knn_dummies = pd.get_dummies(X_train_knn, columns=categorical_vars, drop_first=True, dtype=float)\n",
    "X_test_knn_dummies = pd.get_dummies(X_test_knn, columns=categorical_vars, drop_first=True, dtype=float)\n",
    "\n",
    "# Alinear columnas para asegurar que train y test tengan las mismas \n",
    "X_train_aligned, X_test_aligned = X_train_knn_dummies.align(X_test_knn_dummies, join='inner', axis=1, fill_value=0)\n",
    "\n",
    "# Escalar las características\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_aligned)\n",
    "X_test_scaled = scaler.transform(X_test_aligned)\n",
    "\n",
    "# Iterar sobre los valores de K, entrenar y evaluar el modelo\n",
    "k_values = [1, 5, 10]\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"\\n--- Resultados para K = {k} ---\")\n",
    "    \n",
    "    # Instanciar el modelo\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    \n",
    "    # Entrenar el modelo\n",
    "    knn.fit(X_train_scaled, y_train_knn)\n",
    "    \n",
    "    # Predecir en el conjunto de prueba\n",
    "    y_pred_knn = knn.predict(X_test_scaled)\n",
    "    \n",
    "    # Evaluar el modelo\n",
    "    accuracy = accuracy_score(y_test_knn, y_pred_knn)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"Reporte de Clasificación:\")\n",
    "    print(classification_report(y_test_knn, y_pred_knn, target_names=['No Pobre (0)', 'Pobre (1)']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Búsqueda del K Óptimo con Cross-Validation \n",
    "\n",
    "# Definir el rango de valores de K para probar\n",
    "k_range = range(1, 11)\n",
    "cv_scores = []\n",
    "\n",
    "# Realizar validación cruzada de 5 folds para cada valor de K\n",
    "print(\"Calculando el accuracy promedio para cada K...\")\n",
    "for k in k_range:\n",
    "    knn_cv = KNeighborsClassifier(n_neighbors=k)\n",
    "    # Usamos X_train_scaled y y_train_knn, que son nuestros datos de entrenamiento\n",
    "    scores = cross_val_score(knn_cv, X_train_scaled, y_train_knn, cv=5, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "# Encontrar el K óptimo\n",
    "optimal_k = k_range[np.argmax(cv_scores)]\n",
    "max_accuracy = max(cv_scores)\n",
    "\n",
    "\n",
    "# 4. Graficar los resultados\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, cv_scores, marker='o', linestyle='--', color='b', label='Accuracy de CV')\n",
    "plt.axvline(optimal_k, color='r', linestyle='-', label=f'K Óptimo = {optimal_k}')\n",
    "plt.title('Accuracy de KNN vs. Número de Vecinos (K) - Cross-Validation', fontsize=14)\n",
    "plt.xlabel('Número de Vecinos (K)', fontsize=12)\n",
    "plt.ylabel('Accuracy Promedio (5-Fold CV)', fontsize=12)\n",
    "plt.xticks(k_range)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 5. Comentario sobre el resultado\n",
    "print(\"\\n--- Comentario sobre el K Óptimo ---\")\n",
    "print(f\"El gráfico muestra el rendimiento del modelo para diferentes valores de K. Un K bajo (como 1) puede llevar a un sobreajuste, ya que el modelo es muy sensible a puntos individuales y ruido. A medida que K aumenta, el modelo se generaliza mejor, y el accuracy tiende a subir.\")\n",
    "print(f\"En este caso, el accuracy más alto se alcanza con K={optimal_k}. A partir de este punto, aumentar K podría hacer que el modelo sea demasiado simple (underfitting), perdiendo detalles importantes y disminuyendo su rendimiento.\")\n",
    "print(f\"Por lo tanto, {optimal_k} es el número óptimo de vecinos cercanos para este problema, ya que representa el mejor balance entre sesgo y varianza según la validación cruzada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte D - Modelo de Regresión Logistica con Regularización: Ridge y LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Regularización con LASSO y Ridge ---\n",
    "\n",
    "# 1. Definir la grilla de parámetros de penalidad\n",
    "lambdas = np.logspace(-5, 5, 11)\n",
    "\n",
    "# En sklearn, C es el inverso de la fuerza de penalidad (C = 1/λ)\n",
    "cs = 1 / lambdas\n",
    "\n",
    "coefs_lasso = []\n",
    "coefs_ridge = []\n",
    "\n",
    "# Usamos los datos escalados del entrenamiento de KNN, que ya están preparados\n",
    "\n",
    "# Iteramos para LASSO\n",
    "for c in cs:\n",
    "    # solver='liblinear' es una buena opción para datasets pequeños y funciona con L1 y L2\n",
    "    model_lasso = LogisticRegression(penalty='l1', C=c, solver='liblinear', random_state=random_seed)\n",
    "    model_lasso.fit(X_train_scaled, y_train_knn)\n",
    "    coefs_lasso.append(model_lasso.coef_[0])\n",
    "\n",
    "# Iteramos para Ridge\n",
    "for c in cs:\n",
    "    model_ridge = LogisticRegression(penalty='l2', C=c, solver='liblinear', random_state=random_seed)\n",
    "    model_ridge.fit(X_train_scaled, y_train_knn)\n",
    "    coefs_ridge.append(model_ridge.coef_[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficamos los coeficientes en dos paneles\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8), sharey=True)\n",
    "\n",
    "# Nombres de las variables para las leyendas\n",
    "feature_names = X_train_aligned.columns\n",
    "\n",
    "# Panel para LASSO \n",
    "ax1.plot(np.log10(lambdas), coefs_lasso)\n",
    "ax1.set_xlabel('log10(λ)', fontsize=12)\n",
    "ax1.set_ylabel('Coeficientes', fontsize=12)\n",
    "ax1.set_title('Coeficientes de Regresión Logística con Penalidad LASSO (L1)', fontsize=14)\n",
    "ax1.grid(True)\n",
    "ax1.legend(feature_names, loc='upper right')\n",
    "\n",
    "# Panel para Ridge \n",
    "ax2.plot(np.log10(lambdas), coefs_ridge)\n",
    "ax2.set_xlabel('log10(λ)', fontsize=12)\n",
    "ax2.set_title('Coeficientes de Regresión Logística con Penalidad Ridge (L2)', fontsize=14)\n",
    "ax2.grid(True)\n",
    "ax2.legend(feature_names, loc='upper right')\n",
    "\n",
    "plt.suptitle('Evolución de Coeficientes vs. Fuerza de Penalidad (λ)', fontsize=16, y=1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Penalidad Lasso\n",
    "\n",
    "lasso_cv_model = LogisticRegressionCV(\n",
    "    Cs=cs,\n",
    "    cv=5,\n",
    "    penalty='l1',\n",
    "    solver='liblinear',\n",
    "    scoring='accuracy',\n",
    "    random_state=random_seed,\n",
    "    max_iter=1000, # Aumentar iteraciones para asegurar convergencia\n",
    "    refit=True # Re-entrenar con el mejor C para obtener el C_ óptimo\n",
    ").fit(X_train_scaled, y_train_knn)\n",
    "\n",
    "# Para obtener los scores y coefs de cada fold, necesitamos un modelo sin refit\n",
    "lasso_cv_scores_and_coefs = LogisticRegressionCV(\n",
    "    Cs=cs, cv=5, penalty='l1', solver='liblinear', scoring='accuracy', random_state=random_seed, max_iter=1000, refit=False\n",
    ").fit(X_train_scaled, y_train_knn)\n",
    "\n",
    "\n",
    "# El atributo scores_ es un dict, tomamos los scores de la clase positiva\n",
    "lasso_errors = 1 - lasso_cv_scores_and_coefs.scores_[1]\n",
    "\n",
    "# Calculamos la proporción de coeficientes cero para cada fold y cada C\n",
    "prop_zeros_lasso = np.mean(lasso_cv_scores_and_coefs.coefs_paths_[1] == 0, axis=2)\n",
    "\n",
    "lambda_optimo_lasso = 1 / lasso_cv_model.C_[0]\n",
    "print(f\"El λ^cv óptimo seleccionado para LASSO es: {lambda_optimo_lasso:.5f}\")\n",
    "\n",
    "# Penalidad Ridge \n",
    "\n",
    "ridge_cv_model = LogisticRegressionCV(\n",
    "    Cs=cs, cv=5, penalty='l2', solver='liblinear', scoring='accuracy', random_state=random_seed, max_iter=1000, refit=True\n",
    ").fit(X_train_scaled, y_train_knn)\n",
    "\n",
    "ridge_cv_scores = LogisticRegressionCV(\n",
    "    Cs=cs, cv=5, penalty='l2', solver='liblinear', scoring='accuracy', random_state=random_seed, max_iter=1000, refit=False\n",
    ").fit(X_train_scaled, y_train_knn)\n",
    "\n",
    "ridge_errors = 1 - ridge_cv_scores.scores_[1]\n",
    "lambda_optimo_ridge = 1 / ridge_cv_model.C_[0]\n",
    "print(f\"El λ^cv óptimo seleccionado para Ridge es: {lambda_optimo_ridge:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generación de Box-plots  \n",
    "\n",
    "# Los 'Cs' se ordenan de menor a mayor en el objeto CV, por lo que los lambdas correspondientes van de mayor a menor.\n",
    "log_lambdas_sorted = np.log10(1 / lasso_cv_model.Cs_)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(24, 7), gridspec_kw={'width_ratios': [1, 1, 1]})\n",
    "\n",
    "labels = [f'{l:.1f}' for l in log_lambdas_sorted]\n",
    "\n",
    "# Box-plot de error para LASSO (cada caja corresponde a un λ por eso transponemos)\n",
    "axes[0].boxplot(lasso_errors.T.tolist())\n",
    "axes[0].set_xticklabels(labels, rotation=45)\n",
    "axes[0].set_title('Error de Clasificación (LASSO L1)', fontsize=14)\n",
    "axes[0].set_xlabel('log10(λ)', fontsize=12)\n",
    "axes[0].set_ylabel('Error de Clasificación (1 - Accuracy)', fontsize=12)\n",
    "axes[0].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Box-plot de error para Ridge\n",
    "axes[1].boxplot(ridge_errors.T.tolist())\n",
    "axes[1].set_xticklabels(labels, rotation=45)\n",
    "axes[1].set_title('Error de Clasificación (Ridge L2)', fontsize=14)\n",
    "axes[1].set_xlabel('log10(λ)', fontsize=12)\n",
    "axes[1].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Box-plot de proporción de ceros para LASSO\n",
    "axes[2].boxplot(prop_zeros_lasso.T.tolist())\n",
    "axes[2].set_xticklabels(labels, rotation=45)\n",
    "axes[2].set_title('Proporción de Coeficientes Cero (LASSO L1)', fontsize=14)\n",
    "axes[2].set_xlabel('log10(λ)', fontsize=12)\n",
    "axes[2].set_ylabel('Proporción de Coeficientes = 0', fontsize=12)\n",
    "axes[2].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Añadir marcadores del λ óptimo en los boxplots \n",
    "opt_log_lasso = np.log10(lambda_optimo_lasso)\n",
    "opt_log_ridge = np.log10(lambda_optimo_ridge)\n",
    "\n",
    "# Encontrar la posición del tick más cercano\n",
    "pos_lasso = int(np.argmin(np.abs(log_lambdas_sorted - opt_log_lasso))) + 1\n",
    "pos_ridge = int(np.argmin(np.abs(log_lambdas_sorted - opt_log_ridge))) + 1\n",
    "\n",
    "# Líneas verticales y etiquetas\n",
    "axes[0].axvline(pos_lasso, color='red', linestyle='--', linewidth=1.5)\n",
    "axes[0].text(pos_lasso, axes[0].get_ylim()[1]*0.95, f'λ_opt={lambda_optimo_lasso:.2g}', color='red', ha='center', fontsize=10)\n",
    "\n",
    "axes[1].axvline(pos_ridge, color='red', linestyle='--', linewidth=1.5)\n",
    "axes[1].text(pos_ridge, axes[1].get_ylim()[1]*0.95, f'λ_opt={lambda_optimo_ridge:.2g}', color='red', ha='center', fontsize=10)\n",
    "\n",
    "plt.suptitle('Distribución del Error y Selección de Variables vs. Penalidad λ', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Estimaciones comparativas: sin penalidad (statsmodels), L1 y L2 (sklearn) ---\n",
    "# Usamos: X_train_const (con const) y X_train_dummies (sin const) y result (statsmodels ya ajustado)\n",
    "\n",
    "# Valores de C seleccionados en CV (ya calculados)\n",
    "C_l1 = lasso_cv_model.C_[0]\n",
    "C_l2 = ridge_cv_model.C_[0]\n",
    "\n",
    "# Preparar datos para penalizados: usar las dummies (sin const) y escalar\n",
    "X_pen = X_train_dummies.copy()  # variables en la misma orden que el modelo sin penalidad (sin const)\n",
    "y_pen = y_train  \n",
    "\n",
    "scaler_pen = StandardScaler()\n",
    "X_pen_scaled = scaler_pen.fit_transform(X_pen)\n",
    "\n",
    "\n",
    "# Ajuste L1 con C encontrado por CV\n",
    "model_l1 = LogisticRegression(penalty='l1', C=C_l1, solver='liblinear', random_state=random_seed, max_iter=2000)\n",
    "model_l1.fit(X_pen_scaled, y_pen)\n",
    "\n",
    "# Ajuste L2 con C encontrado por CV\n",
    "model_l2 = LogisticRegression(penalty='l2', C=C_l2, solver='liblinear', random_state=random_seed, max_iter=2000)\n",
    "model_l2.fit(X_pen_scaled, y_pen)\n",
    "\n",
    "# Desescalar coeficientes para volver a la escala original:\n",
    "def unscale_coef(logreg, scaler, feature_names):\n",
    "    b_scaled = logreg.coef_.ravel()\n",
    "    mean = scaler.mean_\n",
    "    scale = scaler.scale_\n",
    "    # coef en escala original\n",
    "    b_unscaled = b_scaled / scale\n",
    "    # intercept en escala original\n",
    "    intercept_unscaled = logreg.intercept_[0] - np.sum(b_scaled * mean / scale)\n",
    "    s = pd.Series(index=list(feature_names) + ['const'], dtype=float)\n",
    "    # poner coeficientes y const en el mismo orden que X_train_const \n",
    "    for i, fn in enumerate(feature_names):\n",
    "        s[fn] = b_unscaled[i]\n",
    "    s['const'] = intercept_unscaled\n",
    "    return s\n",
    "\n",
    "s_l1 = unscale_coef(model_l1, scaler_pen, X_pen.columns)\n",
    "s_l2 = unscale_coef(model_l2, scaler_pen, X_pen.columns)\n",
    "\n",
    "# Serie de coeficientes sin penalidad \n",
    "s_unpen = result.params.copy()\n",
    "# Asegurar mismo orden de índices\n",
    "all_index = X_train_const.columns\n",
    "\n",
    "# Construir DataFrame final con las tres columnas\n",
    "df_coefs = pd.DataFrame(index=all_index)\n",
    "df_coefs['Unpenalized'] = s_unpen.reindex(all_index)\n",
    "df_coefs['L1 (λ^cv)'] = s_l1.reindex(all_index)\n",
    "df_coefs['L2 (λ^cv)'] = s_l2.reindex(all_index)\n",
    "\n",
    "# Formatear\n",
    "df_coefs = df_coefs.round(5)\n",
    "\n",
    "# Exportar a docx \n",
    "doc_coefs = Document()\n",
    "doc_coefs.add_heading('Coeficientes: Sin penalidad vs L1 (λ^cv) vs L2 (λ^cv)', level=1)\n",
    "\n",
    "t = doc_coefs.add_table(rows=1, cols=1 + df_coefs.shape[1])  # primera columna para nombre de variable\n",
    "hdr = t.rows[0].cells\n",
    "hdr[0].text = 'Variable'\n",
    "for j, col in enumerate(df_coefs.columns, start=1):\n",
    "    hdr[j].text = col\n",
    "\n",
    "for var, row in df_coefs.iterrows():\n",
    "    cells = t.add_row().cells\n",
    "    cells[0].text = str(var)\n",
    "    for j, val in enumerate(row, start=1):\n",
    "        cells[j].text = f\"{val:.5f}\" if not pd.isna(val) else \"\"\n",
    "\n",
    "# Ajustar tamaño de fuente\n",
    "for row in t.rows:\n",
    "    for cell in row.cells:\n",
    "        for paragraph in cell.paragraphs:\n",
    "            for run in paragraph.runs:\n",
    "                run.font.size = Pt(9)\n",
    "\n",
    "doc_coefs.save('Coeficientes_Logit_Comparativo.docx')\n",
    "\n",
    "# Resumen\n",
    "print(\"\\nTabla de coeficientes creada: 'Coeficientes_Logit_Comparativo.docx'\")\n",
    "print(df_coefs.head(12))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "name": "TP1 - Parte 1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "primernetorno",
   "language": "python",
   "name": "primernetorno"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
